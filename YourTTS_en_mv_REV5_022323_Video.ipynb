{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsengduong/VCTK_VITS/blob/master/YourTTS_en_mv_REV5_022323_Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HkOd4jwqIb"
      },
      "source": [
        "**Fine tune a Your-TTS model with the Coqui TTS framework using audio samples of your choice.**\n",
        "\n",
        "Thank you to all of the [Coqui TTS](https://https://github.com/coqui-ai/TTS) contributors, the OpenAI team, whoever authored the rnnoise script, and [@Thorsten-Voice](https://https://www.youtube.com/c/ThorstenMueller ) for his helpful videos.\n",
        "\n",
        "The Your-TTS training loop and configuration come from the training recipe by Edresson and iamkhalidbashir.  After cloning the Github repo, you can find it in TTS/recipes/vctk/yourtts/train_yourtts.py\n",
        "\n",
        "[-nn](https://https://www.youtube.com/c/NanoNomad )\n",
        "\n",
        "If you have run the script before, and want to examine your training session using Tensorboard, set your dataset directories, then skip to the bottom section to load Tensorboard.\n",
        "\n",
        "This notebook is currently limited to training English voices.  If you want to try other languages, or multi-speaker datasets, you're on your own for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies recently broke.  Run this cell first, then click RESTART RUNTIME within the output box below. When your session reconnects, continue on with the notebook from the Connect Google Drive cell.**"
      ],
      "metadata": {
        "id": "s5FuNHD2No_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TTS"
      ],
      "metadata": {
        "id": "h0Y_01vuNobd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOoSyCc8h4WI"
      },
      "source": [
        "**Run this cell to connect your Google Drive account. Samples, datasets and tuned models will be saved here.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RUb7_iJu1mb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0B3tVbPAKc"
      },
      "source": [
        "**Variable names**\n",
        "Name your dataset/dataset directory. Single words, no spaces.\n",
        "\n",
        "Set dataset and trainer output directory for saving model states and logs.  This will be a subdirectory of your dataset directory.  If the dataset directory does not exist, it will be created.\n",
        "\n",
        "Set the sample upload directory, then place each voice's wav/mp3 files in separate folders, one for each voice, under this directory.\n",
        "\n",
        "Example:\n",
        "sampleuploads/<br>\n",
        "->sampleuploads/bob/[all bob's audio files here]<br>\n",
        "->sampleuploads/jerry/[all jerry's audio files here]<br>\n",
        "->sampleuploads/tom/[all tom's audio files here]<br>\n",
        "\n",
        "Set your run type.  Restore run begins a new fine tuning session using the model named here (downloaded using Coqui later).  Continue run continues a previous, interrupted fine tuning run (You will select the run to continue later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqNvXLc9ltKb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "ds_name = \"deck-ds\" #@param {type:\"string\"}\n",
        "output_directory = \"traineroutput\" #@param {type:\"string\"}\n",
        "upload_dir = \"yourtts-sampleuploads\" #@param {type:\"string\"}\n",
        "MODEL_FILE = \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\" #@param {type:\"string\"}\n",
        "upload_dir = \"/content/drive/MyDrive/\" + upload_dir\n",
        "RUN_NAME = \"YourTTS-EN-VCTK\" #@param {type:\"string\"}\n",
        "\n",
        "OUT_PATH = \"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"\n",
        "!mkdir $upload_dir\n",
        "!mkdir /content/drive/MyDrive/$ds_name\n",
        "!mkdir /content/drive/MyDrive/$ds_name/txt/\n",
        "!mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_type = \"restore\" #@param [\"continue\",\"restore\",\"restore-ckpt\",\"newmodel\"]\n",
        "print(run_type + \" run selected\")"
      ],
      "metadata": {
        "id": "nSrZbKCXxalg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIvwEf9zydzN"
      },
      "source": [
        "**Download and Build Rnnoise (https://github.com/xiph/rnnoise) and Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HigwgxZxEXf",
        "outputId": "7c944a03-fc21-4a56-d349-0a51cf70fb48",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyloudnorm\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from pyloudnorm) (0.16.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from pyloudnorm) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.8/dist-packages (from pyloudnorm) (1.22.4)\n",
            "Installing collected packages: pyloudnorm\n",
            "Successfully installed pyloudnorm-0.1.1\n",
            "Cloning into 'rnnoise'...\n",
            "remote: Enumerating objects: 420, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 420 (delta 194), reused 192 (delta 192), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (420/420), 836.04 KiB | 4.04 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n",
            "autoconf is already the newest version (2.69-11.1).\n",
            "autoconf set to manually installed.\n",
            "automake is already the newest version (1:1.16.1-4ubuntu6).\n",
            "automake set to manually installed.\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu4).\n",
            "curl is already the newest version (7.68.0-1ubuntu2.15).\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libpython2-dev libpython2-stdlib\n",
            "  libpython2.7 libpython2.7-dev libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "  python-is-python2 python2 python2-dev python2-minimal python2.7-dev\n",
            "Suggested packages:\n",
            "  libsox-fmt-all libtool-doc gcj-jdk python2-doc python-tk\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libpython2-dev libpython2-stdlib\n",
            "  libpython2.7 libpython2.7-dev libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "  libtool python-dev-is-python2 python-is-python2 python2 python2-dev\n",
            "  python2-minimal python2.7-dev sox\n",
            "0 upgraded, 17 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 4,544 kB of archives.\n",
            "After this operation, 20.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7 amd64 2.7.18-1~20.04.3 [1,037 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-dev amd64 2.7.18-1~20.04.3 [2,466 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-dev amd64 2.7.17-2ubuntu4 [7,140 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsox3 amd64 14.4.2+git20190427-2 [226 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2 [10.5 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2 [31.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libtool all 2.4.6-14 [161 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-is-python2 all 2.7.17-4 [2,496 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-dev amd64 2.7.18-1~20.04.3 [293 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-dev amd64 2.7.17-2ubuntu4 [1,268 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-dev-is-python2 all 2.7.17-4 [1,396 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 sox amd64 14.4.2+git20190427-2 [102 kB]\n",
            "Fetched 4,544 kB in 4s (1,136 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 17.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 128155 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../02-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libpython2.7:amd64.\n",
            "Preparing to unpack .../03-libpython2.7_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking libpython2.7:amd64 (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package libpython2.7-dev:amd64.\n",
            "Preparing to unpack .../04-libpython2.7-dev_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking libpython2.7-dev:amd64 (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package libpython2-dev:amd64.\n",
            "Preparing to unpack .../05-libpython2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-base_14.4.2+git20190427-2_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../09-libtool_2.4.6-14_all.deb ...\n",
            "Unpacking libtool (2.4.6-14) ...\n",
            "Selecting previously unselected package python-is-python2.\n",
            "Preparing to unpack .../10-python-is-python2_2.7.17-4_all.deb ...\n",
            "Unpacking python-is-python2 (2.7.17-4) ...\n",
            "Selecting previously unselected package python2.7-dev.\n",
            "Preparing to unpack .../11-python2.7-dev_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking python2.7-dev (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package python2-dev.\n",
            "Preparing to unpack .../12-python2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-dev (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python-dev-is-python2.\n",
            "Preparing to unpack .../13-python-dev-is-python2_2.7.17-4_all.deb ...\n",
            "Unpacking python-dev-is-python2 (2.7.17-4) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../14-sox_14.4.2+git20190427-2_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2) ...\n",
            "Setting up libpython2.7:amd64 (2.7.18-1~20.04.3) ...\n",
            "Setting up libpython2.7-dev:amd64 (2.7.18-1~20.04.3) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2) ...\n",
            "Setting up libtool (2.4.6-14) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2) ...\n",
            "Setting up python-is-python2 (2.7.17-4) ...\n",
            "Setting up python2.7-dev (2.7.18-1~20.04.3) ...\n",
            "Setting up python2-dev (2.7.17-2ubuntu4) ...\n",
            "Setting up python-dev-is-python2 (2.7.17-4) ...\n",
            "Setting up sox (14.4.2+git20190427-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "/content/rnnoise\n",
            "Updating build configuration files for rnnoise, please wait....\n",
            "libtoolize: putting auxiliary files in '.'.\n",
            "libtoolize: linking file './ltmain.sh'\n",
            "libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'm4'.\n",
            "libtoolize: linking file 'm4/libtool.m4'\n",
            "libtoolize: linking file 'm4/ltoptions.m4'\n",
            "libtoolize: linking file 'm4/ltsugar.m4'\n",
            "libtoolize: linking file 'm4/ltversion.m4'\n",
            "libtoolize: linking file 'm4/lt~obsolete.m4'\n",
            "configure.ac:19: installing './compile'\n",
            "configure.ac:27: installing './config.guess'\n",
            "configure.ac:27: installing './config.sub'\n",
            "configure.ac:22: installing './install-sh'\n",
            "configure.ac:22: installing './missing'\n",
            "Makefile.am: installing './depcomp'\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /usr/bin/grep\n",
            "checking for egrep... /usr/bin/grep -E\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking minix/config.h usability... no\n",
            "checking minix/config.h presence... no\n",
            "checking for minix/config.h... no\n",
            "checking whether it is safe to define __EXTENSIONS__... yes\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports the include directive... yes (GNU style)\n",
            "checking whether make supports nested variables... yes\n",
            "checking dependency style of gcc... gcc3\n",
            "checking whether to enable maintainer-specific portions of Makefiles... yes\n",
            "checking for inline... inline\n",
            "checking build system type... x86_64-pc-linux-gnu\n",
            "checking host system type... x86_64-pc-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /usr/bin/sed\n",
            "checking for fgrep... /usr/bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... no\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "checking for a working dd... /usr/bin/dd\n",
            "checking how to truncate binary pipes... /usr/bin/dd bs=4096 count=1\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking whether make supports nested variables... (cached) yes\n",
            "checking if gcc supports -pedantic flag... yes\n",
            "checking if gcc supports -Wall flag... yes\n",
            "checking if gcc supports -Wextra flag... yes\n",
            "checking if gcc supports -Wno-sign-compare flag... yes\n",
            "checking if gcc supports -Wno-parentheses flag... yes\n",
            "checking if gcc supports -Wno-long-long flag... yes\n",
            "checking for cos in -lm... yes\n",
            "checking for gcc way to treat warnings as errors... -Werror\n",
            "checking if gcc supports __attribute__(( visibility(\"default\") ))... yes\n",
            "checking if gcc supports -fvisibility=hidden... yes\n",
            "checking for doxygen... no\n",
            "checking for dot... yes\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating rnnoise.pc\n",
            "config.status: creating rnnoise-uninstalled.pc\n",
            "config.status: creating doc/Doxyfile\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "configure:\n",
            "------------------------------------------------------------------------\n",
            "  rnnoise unknown: Automatic configuration OK.\n",
            "\n",
            "    Assertions ................... no\n",
            "\n",
            "    Hidden visibility ............ yes\n",
            "\n",
            "    API code examples ............ yes\n",
            "    API documentation ............ yes\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "test -z \"librnnoise.la\" || rm -f librnnoise.la\n",
            "rm -f ./so_locations\n",
            "rm -rf .libs _libs\n",
            "rm -rf examples/.libs examples/_libs\n",
            "rm -rf src/.libs src/_libs\n",
            " rm -f examples/rnnoise_demo\n",
            "rm -f *.o\n",
            "rm -f examples/*.o\n",
            "rm -f src/*.o\n",
            "rm -f src/*.lo\n",
            "rm -f *.lo\n",
            "make  all-am\n",
            "make[1]: Entering directory '/content/rnnoise'\n",
            "  CC       examples/rnnoise_demo.o\n",
            "\u001b[01m\u001b[Kexamples/rnnoise_demo.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/rnnoise_demo.c:48:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "   48 |     \u001b[01;35m\u001b[Kfread(tmp, sizeof(short), FRAME_SIZE, f1)\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "  CC       src/denoise.lo\n",
            "  CC       src/rnn.lo\n",
            "  CC       src/rnn_data.lo\n",
            "  CC       src/rnn_reader.lo\n",
            "  CC       src/pitch.lo\n",
            "  CC       src/kiss_fft.lo\n",
            "  CC       src/celt_lpc.lo\n",
            "  CCLD     librnnoise.la\n",
            "  CCLD     examples/rnnoise_demo\n",
            "make[1]: Leaving directory '/content/rnnoise'\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "!pip install pyloudnorm\n",
        "!git clone https://github.com/xiph/rnnoise.git\n",
        "!sudo apt-get install curl autoconf automake libtool python-dev pkg-config sox ffmpeg\n",
        "%cd /content/rnnoise\n",
        "!sh autogen.sh\n",
        "!sh configure\n",
        "!make clean\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "achSZMoMNcCQ"
      },
      "source": [
        "**Install Sox, Install OpenAI Whisper STT+Translation (https://github.com/openai/whisper)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDN_GAyvNbeG",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c3e460-087f-4c6a-a492-743d99d53d40",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "sox is already the newest version (14.4.2+git20190427-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Cloning into 'whisper'...\n",
            "remote: Enumerating objects: 483, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 483 (delta 86), reused 115 (delta 70), pack-reused 343\u001b[K\n",
            "Receiving objects: 100% (483/483), 7.50 MiB | 18.38 MiB/s, done.\n",
            "Resolving deltas: 100% (285/285), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-5lkyel0p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-5lkyel0p\n",
            "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from openai-whisper==20230124) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.16.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->openai-whisper==20230124) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.10)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1179424 sha256=e4267780673a80cdc7c0f4a42fff3a82919935b05e2084930b622ee2fdca955c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-obu4imz3/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, transformers, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.12.1 openai-whisper-20230124 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt install sox\n",
        "!git clone https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzQveIIy7QW"
      },
      "source": [
        "**Install Coqui TTS** (https://github.com/coqui-ai/TTS), espeak-ng phonemeizer (https://github.com/espeak-ng/espeak-ng), download Coqui TTS source and examples from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyCWXW_2y_nx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18134bed-ee8b-4853-ba9b-b7a42f31e11d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "espeak-ng is already the newest version (1.50+dfsg-6).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "fatal: destination path 'TTS' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TTS==0.11.0 in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: gruut[de]==2.2.3 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (2.2.3)\n",
            "Requirement already satisfied: unidic-lite==1.0.8 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.0.8)\n",
            "Requirement already satisfied: mecab-python3==1.0.5 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.0.5)\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.8.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.1.4)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.3.4)\n",
            "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.5.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.12.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.3.5)\n",
            "Requirement already satisfied: cython==0.29.28 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.29.28)\n",
            "Collecting numpy==1.21.6\n",
            "  Using cached numpy-1.21.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (4.64.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (6.0)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (2023.1.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.7.3)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.3.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.42.1)\n",
            "Requirement already satisfied: coqpit>=0.0.16 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.0.17)\n",
            "Requirement already satisfied: numba==0.55.1 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.55.1)\n",
            "Requirement already satisfied: inflect==5.6.0 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (5.6.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: g2pkk>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.1.2)\n",
            "Requirement already satisfied: trainer==0.0.20 in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.0.20)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.4.1)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.48.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (3.2.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (from TTS==0.11.0) (0.13.1+cu116)\n",
            "Requirement already satisfied: dateparser~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (1.1.7)\n",
            "Requirement already satisfied: gruut-ipa<1.0,>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (0.13.0)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (1.2.0)\n",
            "Requirement already satisfied: num2words<1.0.0,>=0.5.10 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (0.5.12)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (2.8.8)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (2.11.0)\n",
            "Requirement already satisfied: gruut-lang-en~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (2.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (5.10.2)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (0.9.9)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gruut[de]==2.2.3->TTS==0.11.0) (2.0.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (0.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (3.0.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0->TTS==0.11.0) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba==0.55.1->TTS==0.11.0) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.8/dist-packages (from numba==0.55.1->TTS==0.11.0) (0.38.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (from trainer==0.0.20->TTS==0.11.0) (2.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from trainer==0.0.20->TTS==0.11.0) (3.19.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from trainer==0.0.20->TTS==0.11.0) (5.4.8)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn==0.5.1->TTS==0.11.0) (0.5.8)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile->TTS==0.11.0) (1.15.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->TTS==0.11.0) (4.5.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->TTS==0.11.0) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask->TTS==0.11.0) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask->TTS==0.11.0) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask->TTS==0.11.0) (2.11.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->TTS==0.11.0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->TTS==0.11.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->TTS==0.11.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->TTS==0.11.0) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->TTS==0.11.0) (2022.6.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->TTS==0.11.0) (2022.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile->TTS==0.11.0) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from dateparser~=1.1.0->gruut[de]==2.2.3->TTS==0.11.0) (1.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask->TTS==0.11.0) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from jsonlines~=1.2.0->gruut[de]==2.2.3->TTS==0.11.0) (1.15.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[de]==2.2.3->TTS==0.11.0) (0.6.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (2.25.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->TTS==0.11.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->gruut[de]==2.2.3->TTS==0.11.0) (3.13.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS==0.11.0) (1.24.3)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.0\n",
            "    Uninstalling numpy-1.20.0:\n",
            "      Successfully uninstalled numpy-1.20.0\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.20\n",
            "  Using cached numpy-1.20.0-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.11.0 requires numpy==1.21.6; python_version < \"3.10\", but you have numpy 1.20.0 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.20.0\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt-get install espeak-ng\n",
        "!git clone https://github.com/coqui-ai/TTS.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional) List pretrained models available on the Coqui Hub**"
      ],
      "metadata": {
        "id": "TZexZefkF1z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tts --list_models"
      ],
      "metadata": {
        "id": "6tLe-D8ucptf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273339ca-003c-453f-a4a0-315c55c8bb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Name format: type/language/dataset/model\n",
            " 1: tts_models/multilingual/multi-dataset/your_tts [already downloaded]\n",
            " 2: tts_models/bg/cv/vits\n",
            " 3: tts_models/cs/cv/vits\n",
            " 4: tts_models/da/cv/vits\n",
            " 5: tts_models/et/cv/vits\n",
            " 6: tts_models/ga/cv/vits\n",
            " 7: tts_models/en/ek1/tacotron2\n",
            " 8: tts_models/en/ljspeech/tacotron2-DDC\n",
            " 9: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
            " 10: tts_models/en/ljspeech/glow-tts\n",
            " 11: tts_models/en/ljspeech/speedy-speech\n",
            " 12: tts_models/en/ljspeech/tacotron2-DCA\n",
            " 13: tts_models/en/ljspeech/vits\n",
            " 14: tts_models/en/ljspeech/vits--neon\n",
            " 15: tts_models/en/ljspeech/fast_pitch\n",
            " 16: tts_models/en/ljspeech/overflow\n",
            " 17: tts_models/en/vctk/vits\n",
            " 18: tts_models/en/vctk/fast_pitch\n",
            " 19: tts_models/en/sam/tacotron-DDC\n",
            " 20: tts_models/en/blizzard2013/capacitron-t2-c50\n",
            " 21: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
            " 22: tts_models/es/mai/tacotron2-DDC\n",
            " 23: tts_models/es/css10/vits\n",
            " 24: tts_models/fr/mai/tacotron2-DDC\n",
            " 25: tts_models/fr/css10/vits\n",
            " 26: tts_models/uk/mai/glow-tts\n",
            " 27: tts_models/uk/mai/vits\n",
            " 28: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
            " 29: tts_models/nl/mai/tacotron2-DDC\n",
            " 30: tts_models/nl/css10/vits\n",
            " 31: tts_models/de/thorsten/tacotron2-DCA\n",
            " 32: tts_models/de/thorsten/vits\n",
            " 33: tts_models/de/thorsten/tacotron2-DDC\n",
            " 34: tts_models/de/css10/vits-neon\n",
            " 35: tts_models/ja/kokoro/tacotron2-DDC\n",
            " 36: tts_models/tr/common-voice/glow-tts\n",
            " 37: tts_models/it/mai_female/glow-tts\n",
            " 38: tts_models/it/mai_female/vits\n",
            " 39: tts_models/it/mai_male/glow-tts\n",
            " 40: tts_models/it/mai_male/vits\n",
            " 41: tts_models/ewe/openbible/vits\n",
            " 42: tts_models/hau/openbible/vits\n",
            " 43: tts_models/lin/openbible/vits\n",
            " 44: tts_models/tw_akuapem/openbible/vits\n",
            " 45: tts_models/tw_asante/openbible/vits\n",
            " 46: tts_models/yor/openbible/vits\n",
            " 47: tts_models/hu/css10/vits\n",
            " 48: tts_models/el/cv/vits\n",
            " 49: tts_models/fi/css10/vits\n",
            " 50: tts_models/hr/cv/vits\n",
            " 51: tts_models/lt/cv/vits\n",
            " 52: tts_models/lv/cv/vits\n",
            " 53: tts_models/mt/cv/vits\n",
            " 54: tts_models/pl/mai_female/vits\n",
            " 55: tts_models/pt/cv/vits\n",
            " 56: tts_models/ro/cv/vits\n",
            " 57: tts_models/sk/cv/vits\n",
            " 58: tts_models/sl/cv/vits\n",
            " 59: tts_models/sv/cv/vits\n",
            " 60: tts_models/ca/custom/vits\n",
            " 61: tts_models/fa/custom/glow-tts\n",
            " Name format: type/language/dataset/model\n",
            " 1: vocoder_models/universal/libri-tts/wavegrad\n",
            " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
            " 3: vocoder_models/en/ek1/wavegrad\n",
            " 4: vocoder_models/en/ljspeech/multiband-melgan\n",
            " 5: vocoder_models/en/ljspeech/hifigan_v2\n",
            " 6: vocoder_models/en/ljspeech/univnet\n",
            " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
            " 8: vocoder_models/en/vctk/hifigan_v2\n",
            " 9: vocoder_models/en/sam/hifigan_v2\n",
            " 10: vocoder_models/nl/mai/parallel-wavegan\n",
            " 11: vocoder_models/de/thorsten/wavegrad\n",
            " 12: vocoder_models/de/thorsten/fullband-melgan\n",
            " 13: vocoder_models/de/thorsten/hifigan_v1\n",
            " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
            " 15: vocoder_models/uk/mai/multiband-melgan\n",
            " 16: vocoder_models/tr/common-voice/hifigan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List folders in sample upload directory**\n",
        "\n",
        "Google Drive can become desyncronized when uploading files with the web interface.  If your sample folder doesn't show up here, wait, or use the desktop application.  If Colab can't see the folder, it can't access the samples."
      ],
      "metadata": {
        "id": "YjWFO5Q7tRLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "%cd $upload_dir\n",
        "!ls -al"
      ],
      "metadata": {
        "id": "uRniRXVjtRkV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the sample uploads subfolder name to process.**\n",
        "\n",
        "Set the name for the new speaker to process.  This will be the speakerid, and stored in the model as VCTK_{name}\n",
        "\n",
        "You can name them both the same thing"
      ],
      "metadata": {
        "id": "BHPyAXuN9p6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subfolder = \"deck\" #@param {type:\"string\"}\n",
        "newspeakername = \"deck\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "g7xei9f-9owi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuK3Veok3zin"
      },
      "source": [
        "**Process**\n",
        "This section will convert mp3 and wav files in upload_dir to  16000hz mono wav files.  Then it will pass the wav files through rnnoise.\n",
        "\n",
        "rnnoise output is then segmented based on 0.2 second silences (click show code below, change 0.2 in the sox line to the duration to silence duration if needed)\n",
        "\n",
        "8000hz Highpass and 50hz lowpass filters applied, gain/loudness adjusted to reduce potential clipping, -6db peak normalization and -25db lufs applied.  Should be fine for general purpose.\n",
        "\n",
        "segmented audio is then passed through sox again to force-split any long segments (above 8 seconds) into segments once again.  Files smaller than 35kb are deleted.\n",
        "\n",
        "These files are then converted to .flac and renamed to correspond to the VCTK dataset format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio Preprocessing Options**\n",
        "\n",
        "Recommended: Leave all 'true'\n",
        "\n",
        "run_denoise use Xiph's rnnoise on samples\n",
        "\n",
        "run_splits split samples based on silence interval of 0.2 seconds and then force a split into 8 second segments. Click view code and find the 'sox' lines to change these intervals.\n",
        "\n",
        "use_audio_filter engage highpass filter 50hz, lowpass fitler 8000hz. Click view code and find the 'sox' lines to change these frequencies if needed.\n",
        "\n",
        "normalize_audio to engage -6db peak -25LUFS normalization"
      ],
      "metadata": {
        "id": "SVmC7zdg4Hbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_denoise = \"True\" #@param [\"True\", \"False\"]\n",
        "run_splits = \"True\" #@param [\"True\", \"False\"]\n",
        "use_audio_filter = \"True\" #@param [\"True\", \"False\"]\n",
        "normalize_audio = \"True\" #@param [\"True\", \"False\"]\n",
        "#start_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#end_sil_dur = 0.2 #@param {type:\"number\"}\n",
        "#sample_max = 8 #@param {type:\"number\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gVoA_9v34HrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import pyloudnorm as pyln\n",
        "import sys\n",
        "import glob\n",
        "%cd $upload_dir\n",
        "%cd $subfolder\n",
        "#Normalize file extensions, convert all to lower case. Use -n flag to test.\n",
        "#https://stackoverflow.com/a/32729737\n",
        "!rename 's/\\.([^.]+)$/.\\L$1/' *\n",
        "\n",
        "#!ls -al\n",
        "!rm -rf $upload_dir/$subfolder/16k_1ch\n",
        "!mkdir $upload_dir/$subfolder/16k_1ch\n",
        "\n",
        "!find . -name '*.mp3' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 16000 -ac 1 16k_1ch/\"${f%.mp3}\".wav ; done' _ {} +\n",
        "!find . -name '*.ogg' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 16000 -ac 1 16k_1ch/\"${f%.ogg}\".wav ; done' _ {} +\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -hide_banner -loglevel error -i \"$f\" -acodec pcm_s16le -ar 16000 -ac 1 16k_1ch/\"${f%.wav}\".wav ; done' _ {} +\n",
        "!ls -al $upload_dir/$subfolder/16k_1ch\n",
        "print(\"Files converted to 16khz 1ch wav\")\n",
        "%cd $upload_dir\n",
        "%cd $subfolder\n",
        "!rm temp.raw\n",
        "!rm rnn.raw\n",
        "%cd 16k_1ch\n",
        "!find . -name \"*.wav\" -type f -size -35k -delete\n",
        "%cd $upload_dir\n",
        "#Convert and resample uploaded mp3/wav clips to 1 channel, 16khz\n",
        "\n",
        "#\n",
        "if run_denoise==\"True\":\n",
        "  print(\"Running denoise...\")\n",
        "  orig_wavs= upload_dir + '/' + subfolder + \"/16k_1ch/\"\n",
        "  print(orig_wavs)\n",
        "\n",
        "  from pathlib import Path\n",
        "  import os\n",
        "  import subprocess\n",
        "  import soundfile as sf\n",
        "  import pyloudnorm as pyln\n",
        "  import sys\n",
        "  import glob\n",
        "  rnn = \"/content/rnnoise/examples/rnnoise_demo\"\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + '/' + subfolder + \"/16k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + '/' + subfolder + \"/16k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "\n",
        "\n",
        "\n",
        "  # Stereo to Mono; upsample to 48000Hz\n",
        "  # added -G to fix gain, -v 0.8\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running highpass/lowpass & resample\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"16000\"])\n",
        "      # apply high/low pass filter and change sr to 16000Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Running resample without filter\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"16000\"])\n",
        "      # apply high/low pass filter and change sr to 16kHz\n",
        "      data, rate = sf.read(target_file)\n",
        "# peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=16000)\n",
        "      print(\"\")\n",
        "    elif normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=16000)\n",
        "      print(\"\")\n",
        "\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "elif run_denoise==\"False\":\n",
        "  paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "  for filepath in paths:\n",
        "    print(\"Skipping denoise...\")\n",
        "    base = os.path.basename(filepath)\n",
        "    tp_s = upload_dir + '/' + subfolder + \"/16k_1ch/denoise/\"\n",
        "    tf_s = upload_dir + '/' + subfolder + \"/16k_1ch/denoise/\" + base\n",
        "    target_path = Path(tp_s)\n",
        "    target_file = Path(tf_s)\n",
        "    print(\"From: \" + str(filepath))\n",
        "    print(\"To: \" + str(target_file))\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "    subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "    #subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "    subprocess.run([\"sox\", \"-G\", \"-v\", \"0.8\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "    subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "    if use_audio_filter==\"True\":\n",
        "      print(\"Running filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"50\", \"lowpass\", \"8000\", \"rate\", \"16000\"]) # apply high/low pass filter and change sr to 160Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "    elif use_audio_filter==\"False\":\n",
        "      print(\"Skipping filter...\")\n",
        "      subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"rate\", \"16000\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "      data, rate = sf.read(target_file)\n",
        "          # peak normalize audio to -6 dB\n",
        "    if normalize_audio==\"True\":\n",
        "      print(\"Output normalized\")\n",
        "      peak_normalized_audio = pyln.normalize.peak(data, -6.0)\n",
        "\n",
        "# measure the loudness first\n",
        "      meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "      loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "      loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "      sf.write(target_file, data=loudness_normalized_audio, samplerate=16000)\n",
        "      print(\"\")\n",
        "    if normalize_audio==\"False\":\n",
        "      print(\"File written without normalizing\")\n",
        "      sf.write(target_file, data=data, samplerate=16000)\n",
        "      print(\"\")\n",
        "  !rm $target_path/rnn.wav\n",
        "  !rm $target_path/48k.wav\n",
        "\n",
        "if run_splits==\"False\":\n",
        "  print(\"Copying files without splitting...\")\n",
        "  %mkdir /content/drive/MyDrive/$ds_name\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "\n",
        "  !cp $target_path/*.wav /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "if run_splits==\"True\":\n",
        "  %mkdir /content/drive/MyDrive/$ds_name\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed\n",
        "  %mkdir /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "  print(\"Splitting output and copying...\")\n",
        "  %cd $target_path\n",
        "  !rm -rf splits\n",
        "  !mkdir splits\n",
        "  !for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress silence 1 0.2 0.1% 1 0.2 0.1% : newfile : restart ; done\n",
        "#alt split method: force splits of 9.5 seconds, however this will split words. Comment the above with # and remove the # below to change\n",
        "#!for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress trim 0 8 : restart ; done\n",
        "  %cd splits\n",
        "  !mkdir resplit\n",
        "  !for FILE in *.wav; do sox \"$FILE\" resplit/\"$FILE\" --show-progress trim 0 9 : newfile : restart ; done\n",
        "  %cd resplit\n",
        "  !find . -name \"*.wav\" -type f -size -35k -delete\n",
        "  #!ls -al\n",
        "  %cd /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "\n",
        "#  !ls -al\n",
        "!cp $target_path/splits/resplit/*.wav /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "%cd /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername\n",
        "!rm *.flac\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -i \"$f\" -c:a flac \"${f%.wav}\"_mic1.flac ; done' _ {} +\n",
        "!rm *.wav\n",
        "!ls /content/drive/MyDrive/$ds_name/wav48_silence_trimmed/$newspeakername"
      ],
      "metadata": {
        "id": "GY2weEjij7dA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Whisper on generated audio clips.**\n",
        "Transcripts will be formatted for the VCTK-style dataset and placed in the<br>\n",
        "->dataset directory<br>\n",
        "---->txt<br>\n",
        "-------->newspeakername<br>"
      ],
      "metadata": {
        "id": "bUwJq6WOjNz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List dataset speaker subdirectories**"
      ],
      "metadata": {
        "id": "7ZycMAWhR-TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!ls /content/drive/MyDrive/$ds_name/wav48_silence_trimmed"
      ],
      "metadata": {
        "id": "psssJnAtRp4-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Whisper STT model. Large-v2 slowest, most accurate, most memory usage. Free Colab users may need to use medium.en.  Select model, run next cell to load it.**\n",
        "\n",
        "**Run the load cell only once. Reloading Whisper STT models may crash your Colab session.**"
      ],
      "metadata": {
        "id": "fSbe43bHRwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_model = \"large-v2\" #@param [\"large-v2\", \"large-v1\", \"medium.en\", \"small.en\", \"base.en\"]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "azm9pXL-9Xuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell once only.\n",
        "\n",
        "Load OpenAI Whisper large-v1 STT model to memory.  Loading it multiple times tends to crash Colab.  \n",
        "\n",
        "Click Show Code below and swap the comment to change models if large-v1 doesn't work on your Colab session."
      ],
      "metadata": {
        "id": "-TXAO2xBTBnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import whisper\n",
        "import os, os.path\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#model = whisper.load_model(\"medium.en\")\n",
        "model = whisper.load_model(whisper_model)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RmD04IXKR9iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the folder to process**"
      ],
      "metadata": {
        "id": "fOuRO5i2SFJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newspeakername = \"deck\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MucYQeY8RlcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run this cell to transcribe clips using Whisper.**\n",
        "\n",
        "To process additional speakers, set a new subfolder/newspeakername above, and remember to re-run that cell before running the one below."
      ],
      "metadata": {
        "id": "l3_UwyvbWJOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPp7FX6viuQe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "wavs = '/content/drive/MyDrive/'+ds_name+'/wav48_silence_trimmed/'+newspeakername\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "if os.path.exists(modelpath):\n",
        "\tif os.path.isfile(modelpath+\"large-v2.pt\"):\n",
        "\t\tprint(\"Saved large-v2 found\")\n",
        "\t\tmodel = whisper.load_model(download_root=modelpath,name=\"large-v2\")\n",
        "else:\n",
        "\tprint(\"loading model from Huggingface\")\n",
        "\tmodel = whisper.load_model(\"large-v2\")\n",
        "\"\"\"\n",
        "paths = glob.glob(os.path.join(wavs, '*.flac'))\n",
        "print(len(paths))\n",
        "all_filenames = []\n",
        "transcript_text = []\n",
        "try:\n",
        "\tos.mkdir('/content/drive/MyDrive/'+ds_name+'/txt/'+newspeakername+'/')\n",
        "except OSError as error:\n",
        "\tprint(error)\n",
        "\n",
        "for filepath in paths:\n",
        "\tbase = os.path.basename(filepath)\n",
        "\tall_filenames.append(base)\n",
        "\n",
        "\tresult = model.transcribe(filepath)\n",
        "\toutput = result[\"text\"].lstrip()\n",
        "\toutput = output.replace(\"\\n\",\"\")\n",
        "\tprint(output)\n",
        "\tthefile = str(os.path.basename(filepath).lstrip(\".\")).rsplit(\".\")[0]\n",
        "\tthefile = thefile[:-5]\n",
        "\tprint(thefile)\n",
        "\toutfilepath = '/content/drive/MyDrive/'+ds_name+'/txt/'+newspeakername+'/'+thefile+'.txt'\n",
        "\twith open(outfilepath, 'w', encoding='utf-8') as indfile:\n",
        "\t\tindfile.write(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the steps above for each new speaker.  You should have a folder arraged like this for your dataset.<br>\n",
        "datasetfolder<br>\n",
        "->txt<br>\n",
        "---->nameone<br>\n",
        "---->nametwo<br>\n",
        "->wav48_silence_trimmed<br>\n",
        "---->nameone<br>\n",
        "---->nametwo<br>"
      ],
      "metadata": {
        "id": "TYtaWUsHej4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before continuing, download your dataset from Google Drive and listen to the audio files, and review the corresponding transcript. If you're not preprocessing with a dataloader (I'm not here), transcribe numbers by hand.\n",
        "\n",
        "Delete any audio files that do not sound good, and their corresponding transcript"
      ],
      "metadata": {
        "id": "_k0dIZmFVQEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 - Training**\n",
        "\n",
        "Download TTS model\n",
        "\n",
        "Load Tensorboard and dashboard\n",
        "\n",
        "Set training variables, load trainer, train"
      ],
      "metadata": {
        "id": "NS2zF2Cxuvnv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX5ftK4TzPUD"
      },
      "source": [
        "**Download Your-TTS model**\n",
        "Generates Sample Wav File to /content/vctk-your-tts.wav  This will be deleted when your Colab session is closed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy-BadvazVNM",
        "outputId": "a2eee26a-4c7f-41fe-9c70-aee49504e015",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n",
            " > External Speaker Encoder Loaded !!\n",
            " > initialization of language-embedding layers.\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n",
            " > Text: flag to one of these values to use the multi-speaker model\n",
            " > Text splitted to sentences.\n",
            "['flag to one of these values to use the multi-speaker model']\n",
            " > Processing time: 1.4113144874572754\n",
            " > Real-time factor: 0.3340389319425504\n",
            " > Saving output to /content/vctk-your-tts.wav\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!tts --text \"flag to one of these values to use the multi-speaker model\" --language_idx en --model_name \"tts_models/multilingual/multi-dataset/your_tts\" --speaker_idx female-en-5 --out_path /content/vctk-your-tts.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ORJUVXnz4B5"
      },
      "source": [
        "**Load Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMqzKCacnPvX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkt32dP3z9sK"
      },
      "source": [
        "**Load Dashboard**\n",
        "May take several minutes to appear from a blank white box.  Ad blockers probably need to whitelist a bunch of Colab stuff or this won't work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxPBHCGf0AVt"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/$ds_name/$output_directory/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Variables**"
      ],
      "metadata": {
        "id": "kVEBYTVDXWpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "import torch\n",
        "from trainer import Trainer, TrainerArgs\n",
        "from TTS.bin.compute_embeddings import compute_embeddings\n",
        "from TTS.bin.resample import resample_files\n",
        "from TTS.config.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsArgs, VitsAudioConfig\n",
        "from TTS.utils.downloaders import download_vctk\n",
        "from TTS.config import load_config\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.utils.managers import save_file\n",
        "from TTS.tts.utils.speakers import SpeakerManager\n",
        "from TTS.tts.utils.data import get_length_balancer_weights\n",
        "from TTS.tts.utils.languages import LanguageManager, get_language_balancer_weights\n",
        "from TTS.tts.utils.speakers import SpeakerManager, get_speaker_balancer_weights, get_speaker_manager\n",
        "#torch.set_num_threads(12)\n",
        "\n",
        "RESTORE_PATH = MODEL_FILE  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n",
        "SKIP_TRAIN_EPOCH = False\n",
        "BATCH_SIZE = 16\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
        "VCTK_DOWNLOAD_PATH = \"/content/drive/MyDrive/\"+ds_name\n",
        "NUM_RESAMPLE_THREADS = 10\n",
        "# Check if VCTK dataset is not already downloaded, if not download it\n",
        "#if not os.path.exists(VCTK_DOWNLOAD_PATH):\n",
        "#    print(\">>> Downloading VCTK dataset:\")\n",
        "#    download_vctk(VCTK_DOWNLOAD_PATH)\n"
      ],
      "metadata": {
        "id": "0JH4DVC3XW1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resample Dataset.**\n",
        "\n",
        "Back up your dataset before running.  Only run once on the dataset."
      ],
      "metadata": {
        "id": "-Lf1FZISZCGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)"
      ],
      "metadata": {
        "id": "6HWcpBIgZCbk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize dataset config.**"
      ],
      "metadata": {
        "id": "pIDVipo_ZdhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "vctk_config = BaseDatasetConfig(\n",
        "    formatter=\"vctk\",\n",
        "    dataset_name=ds_name,\n",
        "    meta_file_train=\"\",\n",
        "    meta_file_val=\"\",\n",
        "    path=VCTK_DOWNLOAD_PATH,\n",
        "    language=\"en-us\",\n",
        "    ignored_speakers=[]\n",
        ")\n",
        "#Add additional datasets\n",
        "#DATASETS_CONFIG_LIST = [vctk_config,vctk_otherdataset,vctk_anotherdataset]\n",
        "DATASETS_CONFIG_LIST = [vctk_config]\n",
        "\n",
        "## Extract speaker embeddings\n",
        "SPEAKER_ENCODER_CHECKPOINT_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
        "\n",
        "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
        "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n",
        "\n"
      ],
      "metadata": {
        "id": "P5A5aFSLZdrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional, may degrade performance, may break things entirely)**\n",
        "\n",
        "Append original vectors from Coqui YourTTS model to attempt to maintain original included speakers."
      ],
      "metadata": {
        "id": "-e7kuKXh0iJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D_VECTOR_FILES.append(\"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/speakers.json\")"
      ],
      "metadata": {
        "id": "UPx0jL_M0iWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it.  Saves file to dataset/speakers.pth**"
      ],
      "metadata": {
        "id": "UUge8N2zZt4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset_conf in DATASETS_CONFIG_LIST:\n",
        "    # Check if the embeddings weren't already computed, if not compute it\n",
        "    print(dataset_conf.path)\n",
        "    embbase=str(dataset_conf.dataset_name)\n",
        "    #embeddings_file = MODEL_DIR+\"speakers.pth\"\n",
        "    embeddings_file = os.path.join(dataset_conf.path, embbase+\"_speakers.pth\")\n",
        "    print(embeddings_file)\n",
        "    if not os.path.isfile(embeddings_file):\n",
        "        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n",
        "        print(SPEAKER_ENCODER_CHECKPOINT_PATH)\n",
        "        print(SPEAKER_ENCODER_CONFIG_PATH)\n",
        "        print(embeddings_file)\n",
        "        print(dataset_conf.formatter)\n",
        "        print(dataset_conf.dataset_name)\n",
        "        print(dataset_conf.path)\n",
        "        print(dataset_conf.meta_file_train)\n",
        "        print(dataset_conf.meta_file_val)\n",
        "        compute_embeddings(\n",
        "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "            SPEAKER_ENCODER_CONFIG_PATH,\n",
        "            embeddings_file,\n",
        "            old_spakers_file=None,\n",
        "            config_dataset_path=None,\n",
        "            formatter_name=dataset_conf.formatter,\n",
        "            dataset_name=dataset_conf.dataset_name,\n",
        "            dataset_path=dataset_conf.path,\n",
        "            meta_file_train=dataset_conf.meta_file_train,\n",
        "            meta_file_val=dataset_conf.meta_file_val,\n",
        "            disable_cuda=False,\n",
        "            no_eval=False,\n",
        "        )\n",
        "    D_VECTOR_FILES.append(embeddings_file)"
      ],
      "metadata": {
        "id": "1yY6I4Xmculj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set VitsAudioConfig and VitsArgs**"
      ],
      "metadata": {
        "id": "ybjEwWD4kFAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio config used in training.\n",
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    hop_length=256,\n",
        "    win_length=1024,\n",
        "    fft_size=1024,\n",
        "    mel_fmin=0.0,\n",
        "    mel_fmax=None,\n",
        "    num_mels=80,\n",
        ")\n",
        "\n",
        "# Init VITSArgs setting the arguments that is needed for the YourTTS model\n",
        "model_args = VitsArgs(\n",
        "    d_vector_file=D_VECTOR_FILES,\n",
        "    use_d_vector_file=True,\n",
        "    d_vector_dim=512,\n",
        "    num_layers_text_encoder=10,\n",
        "    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
        "    resblock_type_decoder=\"2\",  # On the paper, we accidentally trained the YourTTS using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n",
        "    # Usefull parameters to enable the Speaker Consistency Loss (SCL) discribed in the paper\n",
        "    use_speaker_encoder_as_loss=True,\n",
        "    # Usefull parameters to the enable multilingual training\n",
        "    # use_language_embedding=True,\n",
        "    # embedded_language_dim=4,\n",
        ")\n"
      ],
      "metadata": {
        "id": "SJsUFkMmaaI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tweak Config**\n",
        "\n",
        "\n",
        "If training more than one speaker, enable the speaker weighted sampler."
      ],
      "metadata": {
        "id": "Bl_xPs4Va0q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General training config, here you can change the batch size and others usefull parameters\n",
        "config = VitsConfig(\n",
        "    output_path=OUT_PATH,\n",
        "    model_args=model_args,\n",
        "    run_name=RUN_NAME,\n",
        "    project_name=\"YourTTS\",\n",
        "    run_description=\"\"\"YourTTS transfer learning test\"\"\",\n",
        "    dashboard_logger=\"tensorboard\",\n",
        "    logger_uri=None,\n",
        "    audio=audio_config,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    #batch_group_size=16,\n",
        "    batch_group_size=0,\n",
        "    eval_batch_size=BATCH_SIZE,\n",
        "    num_loader_workers=2,\n",
        "    eval_split_max_size=256,\n",
        "    print_step=50,\n",
        "    plot_step=100,\n",
        "    log_model_step=500,\n",
        "    save_all_best=True,\n",
        "    save_step=1000,\n",
        "    save_n_checkpoints=5,\n",
        "    save_checkpoints=True,\n",
        "    target_loss=\"loss_0\",\n",
        "    print_eval=False,\n",
        "    use_phonemes=True,\n",
        "    phonemizer=\"espeak\",\n",
        "    phoneme_language=\"en-us\",\n",
        "    compute_input_seq_cache=True,\n",
        "    add_blank=True,\n",
        "    text_cleaner=\"multilingual_cleaners\",\n",
        "    phoneme_cache_path=\"phoneme_cache\",\n",
        "    precompute_num_workers=2,\n",
        "    start_by_longest=True,\n",
        "    datasets=DATASETS_CONFIG_LIST,\n",
        "    cudnn_benchmark=False,\n",
        "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
        "    mixed_precision=False,\n",
        "    # Enable the weighted sampler\n",
        "    # Set to false if training one speaker.\n",
        "    # Set to true if training 2 or more speakers (optional)\n",
        "    #use_weighted_sampler=True,\n",
        "    use_weighted_sampler=False,\n",
        "    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n",
        "    weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
        "    weighted_sampler_multipliers={\"speaker_name\": {}},\n",
        "    # It defines the Speaker Consistency Loss (SCL) α to 9 like the paper\n",
        "    speaker_encoder_loss_alpha=9.0\n",
        ")"
      ],
      "metadata": {
        "id": "VZWXUNnHa1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manually engage the speaker manager and language manager. Export ID files to dataset directory (speaker_ids.json, speakers.json, language_ids.json. Don't know if this is necessary at the moment, but now you have them.)**"
      ],
      "metadata": {
        "id": "yhOEkx0-4uT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "speaker_manager = SpeakerManager(\n",
        "    d_vectors_file_path=D_VECTOR_FILES,\n",
        "    encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
        "    encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH\n",
        ")\n",
        "\n",
        "print(\"Speakers: \" + str(speaker_manager.get_speakers()))\n",
        "speaker_manager.save_ids_to_file(VCTK_DOWNLOAD_PATH+\"/speaker_ids.json\")\n",
        "print(\"Speaker IDs written to: \" + VCTK_DOWNLOAD_PATH+\"/speaker_ids.json\")\n",
        "speaker_manager.save_embeddings_to_file(VCTK_DOWNLOAD_PATH+\"/speakers.json\")\n",
        "print(\"Speaker embeddings saved to: \" + VCTK_DOWNLOAD_PATH+\"/speakers.json\")\n",
        "language_manager = LanguageManager(config=config)\n",
        "config.model_args.num_languages = language_manager.num_languages\n",
        "print(str(language_manager.num_languages) + \" languages found\")\n",
        "language_manager.save_ids_to_file(VCTK_DOWNLOAD_PATH+\"/language_ids.json\")\n",
        "print(\"Language IDs written to: \" + VCTK_DOWNLOAD_PATH+\"/language_ids.json\")\n",
        "# Load all the datasets samples and split traning and evaluation sets\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    config.datasets,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "#speaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n",
        "config.num_speakers = speaker_manager.num_speakers\n",
        "config.model_args.num_speakers = speaker_manager.num_speakers\n",
        "config.model_args.speakers_file=None\n",
        "config.speakers_file=None\n",
        "config.save_json(VCTK_DOWNLOAD_PATH+\"config.json\")\n",
        "print(\"Config.json written to \"+VCTK_DOWNLOAD_PATH+\"config.json\")\n",
        "#onfig.load_json(VCTK_DOWNLOAD_PATH+\"config.json\")\n",
        "#print(\"Config.json loaded from \"+VCTK_DOWNLOAD_PATH+\"config.json\")"
      ],
      "metadata": {
        "id": "aSDV_qZq4uiK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate test sentences**"
      ],
      "metadata": {
        "id": "QtKN1Z6NezrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_SPEAKERS = []\n",
        "ALL_SPEAKERS = speaker_manager.speaker_names\n",
        "ALL_SENTENCES = []\n",
        "TEST_SENTENCES_PH1 = \"I am the very model of a modern Major-General, I've information vegetable, animal, and mineral,  I know the kings of England, and I quote the fights historical. From Marathon to Waterloo, in order categorical\"\n",
        "TEST_SENTENCES_PH2 = \"I'm sorry, but I don’t want to be an emperor. That's not my business. I don't want to rule or conquer anyone. I should like to help everyone, if possible; Jew, Gentile, black man, white. We all want to help one another. Human beings are like that. We want to live by each other's happiness - not by each other's misery. We don't want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.\"\n",
        "TEST_SENTENCES_PH3 = \"A pot of tea helps to pass the evening.\"\n",
        "TEST_SENTENCES_PH4 = \"Smoky fires lack flame and heat.\"\n",
        "print(ALL_SPEAKERS)\n",
        "for speakername in ALL_SPEAKERS:\n",
        "    ph1 = [TEST_SENTENCES_PH1,speakername,None,\"en-us\"]\n",
        "    ph2 = [TEST_SENTENCES_PH2,speakername,None,\"en-us\"]\n",
        "    ph3 = [TEST_SENTENCES_PH1,speakername,None,\"en-us\"]\n",
        "    ph4 = [TEST_SENTENCES_PH2,speakername,None,\"en-us\"]\n",
        "    ALL_SENTENCES.append(ph1)\n",
        "    ALL_SENTENCES.append(ph2)\n",
        "    ALL_SENTENCES.append(ph3)\n",
        "    ALL_SENTENCES.append(ph4)\n",
        "#print(ALL_SENTENCES)\n",
        "config.test_sentences=ALL_SENTENCES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FRMmrAYexdR",
        "outputId": "38b90f61-e2e1-49d5-c2e1-3e4c550baab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['VCTK_deck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the model**"
      ],
      "metadata": {
        "id": "bgewW9kZ669V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Vits.init_from_config(config)"
      ],
      "metadata": {
        "id": "EcPMGc2E67Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If continuing a run, run the next cell to list all run directories. Copy and paste the run you want to continue into the next box, then run the cell.**"
      ],
      "metadata": {
        "id": "VGuwEFDGBfAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!ls -al /content/drive/MyDrive/$ds_name/traineroutput"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVGDNfudBfaY",
        "outputId": "cc870654-5903-4f17-c497-ef69727f1cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwx------ 2 root root 4096 Jan 26 01:23 YourTTS-EN-VCTK-January-26-2023_01+23AM-0000000\n",
            "drwx------ 2 root root 4096 Jan 26 00:57 YourTTS-EN-VCTK-January-26-2023_12+57AM-0000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run to continue**"
      ],
      "metadata": {
        "id": "Gri9U9LTBnep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_folder = \"YourTTS-EN-VCTK-January-26-2023_01+23AM-0000000\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sx4BQgXeBrAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Init the trainer**"
      ],
      "metadata": {
        "id": "SnRe0PQ37hRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer options"
      ],
      "metadata": {
        "id": "5rvC8wWnejdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current reinit_text_encoder value: \" + str(config.model_args.reinit_text_encoder))\n",
        "reinit_te_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_te_status==\"False\":\n",
        "  config.model_args.reinit_text_encoder=False\n",
        "  print(\"Text encoder will not be reinitilized\")\n",
        "elif reinit_te_status==\"True\":\n",
        "  config.model_args.reinit_text_encoder=True\n",
        "  print(\"Model arguments set to reinitilize text encoder\")\n",
        "  print(\"Current reinit_DP value: \" + str(config.model_args.reinit_DP))\n",
        "reinit_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if reinit_DP_status==\"False\":\n",
        "  config.model_args.reinit_DP=False\n",
        "  print(\"DP will not be reinitilized\")\n",
        "elif reinit_DP_status==\"True\":\n",
        "  config.model_args.reinit_DP=True\n",
        "  print(\"Model arguments set to reinitilize DP\")\n",
        "print(\"Current freeze_waveform_decoder value: \" + str(config.model_args.freeze_waveform_decoder))\n",
        "freeze_waveform_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_waveform_decoder_status==\"False\":\n",
        "  print(\"Waveform decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_waveform_decoder=False\n",
        "elif freeze_waveform_decoder_status==\"True\":\n",
        "  config.model_args.freeze_waveform_decoder=True\n",
        "  print(\"Waveform decoder FROZEN\")\n",
        "print(\"Current freeze_flow_decoder value: \" + str(config.model_args.freeze_flow_decoder))\n",
        "freeze_flow_decoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_flow_decoder_status==\"False\":\n",
        "  print(\"Flow decoder will NOT be frozen\")\n",
        "  config.model_args.freeze_flow_decoder=None\n",
        "elif freeze_flow_decoder_status==\"True\":\n",
        "  config.model_args.freeze_flow_decoder=\"True\"\n",
        "  print(\"Flow decoder FROZEN\")\n",
        "print(\"Current freeze_encoder value: \" + str(config.model_args.freeze_encoder))\n",
        "freeze_encoder_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_encoder_status==\"False\":\n",
        "  print(\"Text encoder will NOT be frozen\")\n",
        "  config.model_args.freeze_encoder=False\n",
        "elif freeze_encoder_status==\"True\":\n",
        "  config.model_args.freeze_encoder=True\n",
        "  print(\"Text encoder FROZEN\")\n",
        "print(\"Current freeze_DP value: \" + str(config.model_args.freeze_DP))\n",
        "freeze_DP_status = \"False\" #@param [\"False\", \"True\"]\n",
        "if freeze_DP_status==\"False\":\n",
        "  print(\"Duration predictor will NOT be frozen\")\n",
        "  config.model_args.freeze_DP=False\n",
        "elif freeze_DP_status==\"True\":\n",
        "  config.model_args.freeze_DP=True\n",
        "  print(\"Duration predictor FROZEN\")"
      ],
      "metadata": {
        "id": "gV4P42IQVyTO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initilize the trainer"
      ],
      "metadata": {
        "id": "xWIp2g0QemK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(run_type)\n",
        "\n",
        "if run_type==\"continue\":\n",
        "  CONTINUE_PATH=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder\n",
        "  trainer = Trainer(\n",
        "    TrainerArgs(continue_path=CONTINUE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore\":\n",
        "    trainer = Trainer(\n",
        "    TrainerArgs(restore_path=MODEL_FILE, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "    config,\n",
        "    output_path=OUT_PATH,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "elif run_type==\"restore-ckpt\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(restore_path=\"/content/drive/MyDrive/\"+ds_name+\"/traineroutput/\"+run_folder+\"/\"+ckpt_file, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        "  )\n",
        "elif run_type==\"newmodel\":\n",
        "  trainer = Trainer(\n",
        "  TrainerArgs(),\n",
        "  config,\n",
        "  output_path=OUT_PATH,\n",
        "  model=model,\n",
        "  train_samples=train_samples,\n",
        "  eval_samples=eval_samples,\n",
        ")"
      ],
      "metadata": {
        "id": "v_Du2F3iajC1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run training**"
      ],
      "metadata": {
        "id": "QuElMKd0p1gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit()"
      ],
      "metadata": {
        "id": "_Y5n6uY-7re_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}